---
title: "Assignments"
output:
  html_document:
    toc: true
    toc_float: true
    collapsed: false
    number_sections: false
    toc_depth: 1
    #code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE,warning=FALSE, cache=TRUE)
```

This page will contain all the assignments you submit for the class.



### Instructions for all assignments

I want you to submit your assignment as a PDF, so I can keep a record of what the code looked like that day. I also want you to include your answers on your personal GitHub website. This will be good practice for editing your website and it will help you produce something you can keep after the class is over.

1. Download the Assignment1.Rmd file from Canvas. You can use this as a template for writing your answers. It's the same as what you can see on my website in the Assignments tab. Once we're done with this I'll edit the text on the website to include the solutions.

2. On RStudio, open a new R script in RStudio (File > New File > R Script). This is where you can test out your R code. You'll write your R commands and draw plots here.

3. Once you have finalized your code, copy and paste your results into this template (Assignment 1.Rmd). For example, if you produced a plot as the solution to one of the problems, you can copy and paste the R code in R markdown by using the ` ``{r} ``` ` command. Answer the questions in full sentences and Save.

4. Produce a PDF file with your answers. To do this, knit to PDF (use Knit button at the top of RStudio), locate the PDF file in your docs folder (it's in the same folder as the Rproj), and submit that on on Canvas in Assignment 1.

5. Build Website, go to GitHub desktop, commit and push. Now your solutions should be on your website as well.






# Assignment 1

**Collaborators: Lorem Ipsum. **

This assignment is due on Canvas on Monday 9/20 before class, at 10:15 am. Include the name of anyone with whom you collaborated at the top of the assignment.


### Problem 1 

Install the datasets package on the console below using `install.packages("datasets")`. Now load the library.

```{r}
library(datasets)
```

Load the USArrests dataset and rename it `dat`. Note that this dataset comes with R, in the package datasets, so there's no need to load data from your computer. Why is it useful to rename the dataset?

It is useful to rename datasets because it gives us a shorthand to work with. So in this case, instead of referring to the data with "USArrests" we can ref to it with dat.

```{r}
dat <- USArrests
```

### Problem 2

Use this command to make the state names into a new variable called State. 

```{r, eval=TRUE}
dat$state <- tolower(rownames(USArrests))
```

This dataset has the state names as row names, so we just want to make them into a new variable. We also make them all lower case, because that will help us draw a map later - the map function requires the states to be lower case.


List the variables contained in the dataset `USArrests`.


```{r}
summary(dat)
names(dat)
```


The four variables are  "Murder", "Assault", "UrbanPop", and "Rape" (and the state variable which we created).

### Problem 3 

What type of variable (from the DVB chapter) is `Murder`? 

Answer: It is a quantitative variable because this variable is representing some numerical value in relation to a state.

What R Type of variable is it?

Answer: This variable is a character because the word murder itself is represented in a string format.


### Problem 4

What information is contained in this dataset, in general? What do the numbers mean? 

Answer: The dataset contains information about murder, assault, and rape. Additionally, it seems to give us some numbers for a states urban population to help see the relation aswell. These numbers show us the relationship with often they are occuring) of these different variables in different states. For example a number for murder is telling it there was some amount of murders within this state (and we can compare this to other states by seeing how much more or less these crimes occur in other states).  

### Problem 5

Draw a histogram of `Murder` with proper labels and title.

```{r}
hist(dat$Murder, main = "Histogram of Murder", xlab = "States")

```

### Problem 6

Please summarize `Murder` quantitatively. What are its mean and median? What is the difference between mean and median? What is a quartile, and why do you think R gives you the 1st Qu. and 3rd Qu.?

```{r}
  summary(dat$Murder)
```

 Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  0.800   4.075   7.250   7.788  11.250  17.400 
  
The mean is 7.788 and the median and 7.250. The mean is the average of the dataset while the median gives us a central value of our dataset. A quartile tells us the variability around the median. So the 1st and 3rd quartiles show us the variability before the median is reached and after the median is reached. R gives us this data to show us where it might be more or less skewed.

### Problem 7

Repeat the same steps you followed for `Murder`, for the variables `Assault` and `Rape`. Now plot all three histograms together. You can do this by using the command `par(mfrow=c(3,1))` and then plotting each of the three. 



Answer (for data on the other two variables) :
For assaults, the mean is 170.8 and the median and 159.0. 

For rapes, the mean is 21.23 and the median and 20.10. 


```{r, echo = TRUE, fig.width = 5, fig.height = 8}
summary(dat$Murder)
summary(dat$Assault)
summary(dat$Rape)



par(mfrow=c(3,1))
hist(dat$Assault, main = "Histogram of Assault", xlab = "States")
hist(dat$Murder, main = "Histogram of Murder", xlab = "States")
hist(dat$Rape, main = "Histogram of Rape", xlab = "States")
```



What does the command par do, in your own words (you can look this up by asking R `?par`)?

Answer: It helps us combine multiple plots that we created into one big vertical plot.

What can you learn from plotting the histograms together?

Answer: We can see the correlation between the data. If there are some points where there is a peak at the same time then we can generalize and say that state could be more dangerous than others. This could work likewise for the converse situation.
  
### Problem 8

In the console below (not in text), type `install.packages("maps")` and press Enter, and then type `install.packages("ggplot2")` and press Enter. This will install the packages so you can load the libraries.

Run this code:

```{r, eval = TRUE, fig.width = 7.5, fig.height = 4}
library('maps') 
library('ggplot2') 

ggplot(dat, aes(map_id=state, fill=Murder)) + 
  geom_map(map=map_data("state")) + 
  expand_limits(x=map_data("state")$long, y=map_data("state")$lat)
```

What does this code do? Explain what each line is doing.

Answer: The first two lines import the libraries map and ggplot2. Line 154 imports dat wants to define the map id of states with murder. The second line is filling out map with state data.The third line maps it out in a x and y axis so we can see the data in states.

# Assignment 2

## Problem 1: Load data

Set your working directory to the folder where you downloaded the data.

```{r}
setwd("/Users/akshatshah/Desktop/upenn/crim250/LabJournal")
```

## Read the data
```{r}
dat <- read.csv(file = 'dat.nsduh.small.1.csv')
```

What are the dimensions of the dataset? 

```{r}
dim(dat)
```

Answer: There are 171 rows and 7 columns in this dataset.

```{r}
names(dat)
```

## Problem 2: Variables

 Describe the variables in the dataset.

The variables are mjage, ciage, iralcage, age2, sexatract, speakengl, and irsex.
They each tell us different things. Mjage tells how old someone was when
they first starting using marijuana. Ciage tells how old someone was when
they first starting smoking cigs every day. iralcage tells how old someone
was when they tried alcohol. age2 tells us hpw old the person currently is.
However, this gives us a range because a person could have changes their choice
based on previous responses and the questions they say. Irsex tells us
their gender. sexatract tells us describes their attraction/sexuality.
speakengl tells how well the individual speaks english. 

What is this dataset about? Who collected the data, what kind of sample is it, and what was the purpose of generating the data?

This dataset is a survey about national drug use and health. The data is sponsored by
the United States Health and Human Services. The sample is a scientific random sample
of household addresses. This data gives us a state and nationwide statistics on
drug use. This information is used to help with prevention, trend studies, and inform public
health policy. 

## Problem 3: Age and gender

What is the age distribution of the sample like? Make sure you read the codebook to know what the variable values mean.

The age distribution is more towards the 13-17 year old range. However, when we look at this data we should understand that it can be more of a range since participants could change their answers based on the decisions on they made or what they see fit.

Do you think this age distribution representative of the US population? Why or why  not?

I believe this data not a good representative of the US population.
We are looking at a younger population that makes up around 35% of the age distribution.
So we are leaving out a large majority that can help us see more trends.

Is the sample balanced in terms of gender? If not, are there more females or males?

I believe this data is pretty balanced. There is a pretty even distribution but 
for some of the ages we can see that there is a clear majority like for 17.

Use this code to draw a stacked bar plot to view the relationship between sex and age. What can you conclude from this plot?
```{r}
tab.agesex <- table(dat$irsex, dat$age2)

barplot(tab.agesex,
        main = "Stacked barchart",
        xlab = "Age category", ylab = "Frequency",
        legend.text = rownames(tab.agesex),
        beside = FALSE) # Stacked bars (default)
```



## Problem 4: Substance use

For which of the three substances included in the dataset (marijuana, alcohol, and cigarettes) do individuals tend to use the substance earlier?

```{r}
summary(dat)
```


Looking at the summary of the data, we can see on average what is used earlier on.
Alcohol seems to be used the earliest at an average of 14.95. Then it is mjage at 15.99
Finally, it is cigage at 17.65.


## Problem 5: Sexual attraction

What does the distribution of sexual attraction look like? Is this what you expected?

```{r}
table(dat$sexatract)
```

We see that the largest amount of people (136) chose 1. Yes, this is what I 
expected since it is the norm to be heterosexual.

What is the distribution of sexual attraction by gender? 

```{r}
table(dat$sexatract, dat$irsex)
```

By gender, the distribution is the same in that the majority of both gender
are attracted to the opposite gender. However, more females chose other options.


## Problem 6: English speaking

What does the distribution of English speaking look like in the sample? Is this what you might expect for a random sample of the US population?

```{r}
table(dat$speakengl)
```

The majority of people chose that they speak english very well. And there were
very few who chose well and not well (10 total). This is probably what I would 
expect since it is the dominant language.


Are there more English speaker females or males?

```{r}
table(dat$speakengl, dat$irsex)
```
There are more English speakers who are male than female.

# Exam 1

## Instructions

a. Create a folder in your computer (a good place would be under Crim 250, Exams). 

b. Download the dataset from the Canvas website (fatal-police-shootings-data.csv) onto that folder, and save your Exam 1.Rmd file in the same folder.

c. Download the README.md file. This is the codebook. 

d. Load the data into an R data frame.
```{r}
dat <- read.csv(file = "Crim 250 - Exam 1/fatal-police-shootings-data.csv")
```


## Problem 1 (10 points)

a. Describe the dataset. This is the source: https://github.com/washingtonpost/data-police-shootings . Write two sentences (max.) about this.

This is a dataset that is compiled by the Washington Post of victims of fatal police shootings. At each row, we are given a victim's name and data on the situation that was at hand.

b. How many observations are there in the data frame?
```{r}
dim((dat))
```

We know there are 6594 rows and 17 columns. We know that the number of observations are the
number of rows. Therefore there are 6593 observations (not including the first row
since this is the title of the columns).

c. Look at the names of the variables in the data frame. Describe what "body_camera", "flee", and "armed" represent, according to the codebook. Again, only write one sentence (max) per variable.
```{r}

names(dat)
```

Body camera is a variable that is telling us if an officer was wearing a body 
camera and if it was recording what happened.

Flee is a variable that was indicating if the victim was moving away, and if they were fleeing this tells us by what method.

Armed is a variable that tells if the officer believe they had some tool that could inflict damage.

d. What are three weapons that you are surprised to find in the "armed" variable? Make a table of the values in "armed" to see the options.
```{r}

table(dat$armed)

```

I am suprised to see pen, binoculars, and contractor's level.

## Problem 2 (10 points)

a. Describe the age distribution of the sample. Is this what you would expect to see?
```{r}
hist(dat$age, main = "Histogram of Age Distribution", xlab = "Age (in years)", xlim = c(0, 100))
```

This distribution is skewed to the right. This isn't exactly what I expected,
(I thought it would be skewed even more to the right.) because I believed that victims would
be a lot younger.

b. To understand the center of the age distribution, would you use a mean or a median, and why? Find the one you picked.
```{r}
summary(dat$age)

```

I would use mean to understand the center of the age distribution because I don't believe there are enough outliers/extremes that would skew this result substantially. The mean
gives us an average over our data. We can see that the average age of victims for 
police fatal shootings were 37.12 years old.

c. Describe the gender distribution of the sample. Do you find this surprising?
```{r}

table(dat$gender)

```

Within this dataset, there are significantly more men than women (6005 more). This is not surprising because statistics have shown that men have been higher arrest rate than women. Since men have a higher arrest rate than women and more encounters with police, I expected there to be more men than women within this dataset. Additonally, there were 3 blank indexes
for this data and it was not taken into account for the calculations above since it
is inconclusive.


## Problem 3 (10 points)

a. How many police officers had a body camera, according to news reports? What proportion is this of all the incidents in the data? Are you surprised that it is so high or low?

```{r}
table(dat$body_camera)

```

Only 910 officers had a body camera according to this dataset. This means that 
only 14% of incidents had a body camera. This is extremely suprising that is so low
because people are losing their lives and a proportion of officers have no concrete 
evidence of the situation due to no body camera. 

b. In  how many of the incidents was the victim fleeing? What proportion is this of the total number of incidents in the data? Is this what you would expect?
```{r}
table(dat$flee)


```

Out of 6103 incidents that recorded something in the fleeing category, 2151 victims were 
fleeing. This means about 35% of people who were a victim of a fatal police shooting 
were fleeing. I suspected a larger proportion of people were fleeing, but this 
dataset refutes that idea. Additionally, there is 491 indexes of data that are blank
for fleeing, so this was removed from the total number of incidents and the proportion
since it is inconclusive.


## Problem 4 (10 points) -  Answer only one of these (a or b).

a. Describe the relationship between the variables "body camera" and "flee" using a stacked barplot. What can you conclude from this relationship? 

*Hint 1: The categories along the x-axis are the options for "flee", each bar contains information about whether the police officer had a body camera (vertically), and the height along the y-axis shows the frequency of that category).*

*Hint 2: Also, if you are unsure about the syntax for barplot, run ?barplot in R and see some examples at the bottom of the documentation. This is usually a good way to look up the syntax of R code. You can also Google it.*


```{r}



```

__Your answer here.__

b. Describe the relationship between age and race by using a boxplot. What can you conclude from this relationship? 

*Hint 1: The categories along the x-axis are the race categories and the height along the y-axis is age.* 

*Hint 2: Also, if you are unsure about the syntax for boxplot, run ?boxplot in R and see some examples at the bottom of the documentation. This is usually a good way to look up the syntax of R code. You can also Google it.*


```{r}

boxplot(dat$age~factor(dat$race), ylab = "Age", xlab = "Race")

```

We see from the data that some races have a slightly lower mean age than others.
For examples, the mean age for white people looks to be around 40, while the mean 
age for black people seems to be about 35. Additionally, some races have a lot more
outliers than others. For example, asian people have no outliers while black people
have a lot of outliers. It is hard to make a concrete conclusion based on this relationship, 
but we can observe that there are clear small differences that we can observe
(as stated above) from the different races and their ages. Furthermore, the first category simple represent people who did not have a specified age (left blank in the dataset). I did not omit this data because I think it could still be important to see this in relation to the other
data.


## Extra credit (10 points)

a. What does this code tell us? 

```{r, eval=FALSE}
mydates <- as.Date(dat$date)
head(mydates)
(mydates[length(mydates)] - mydates[1])
```

This data tells us how long it has been since the first entry within this dataset
to the most recent entry within the dataset. We are taking the first index because
the 0th index states the name of each column, and we are taking the last index to get the last entry. The difference is 2458 days which is about 6.7 years. This makes sense because this dataset was created in 2015 and we are about 6 and 3/4 years from this time.

b. On Friday, a new report was published that was described as follows by The Guardian: "More than half of US police killings are mislabelled or not reported, study finds." Without reading this article now (due to limited time), why do you think police killings might be mislabelled or underreported?

I believe this is because of bias. Police who have been apart of police killings are going 
to defend themselves. In order to do so, they might underreport or mislabel the incident
in order to save face and justify the actions they took. Additionally, co workers of 
police who have been apart of such an incident may look to defend each other building
even more of a bias.

c. Regarding missing values in problem 4, do you see any? If so, do you think that's all that's missing from the data?

In problem 4, there were clear missing values (race was not defined). I believe there 
is more missing from the data. If we were to look closer into the data we can see that 
sometimes, for example, gender isn't specified. Additionally, another data that
has missing values is fleeing (491 are blank). 

# Exam 2

## Instructions

a. Create a folder in your computer (a good place would be under Crim 250, Exams). 

b. Download the dataset from the Canvas website (sim.data.csv) onto that folder, and save your Exam 2.Rmd file in the same folder.

c. Data description: This dataset provides (simulated) data about 200 police departments in one year. It contains information about the funding received by the department as well as incidents of police brutality. Suppose this dataset (sim.data.csv) was collected by researchers to answer this question: **"Does having more funding in a police department lead to fewer incidents of police brutality?"**
d. Codebook:
- funds: How much funding the police department received in that year in millions of dollars.
- po.brut: How many incidents of police brutality were reported by the department that year.
- po.dept.code: Police department code

## Problem 1: EDA (10 points) 

Describe the dataset and variables. Perform exploratory data analysis for the two variables of interest: funds and po.brut.

```{r}
dat <- read.csv(file = 'sim.data.csv')
names(dat)
summary(dat)
dim(dat)
plot(dat$funds, dat$po.brut,  main="Relationship between Police Department Funding (in millions) and Police Bruality (per year)", xlab="Amount of funding", ylab="Number of police brutality incidences per year")
cor(dat$funds, dat$po.brut)
```

The dataset has three different variables consisting of the department code,
funds, and police brutality. There are 200 observations of different police
departments and the number of incidences where police brutality occurred. We 
see that on average a department gets around 61 million dollars in funding and
will have about 18 incidences of police brutality that year. We can see from
the graph that there is a clear decrease in the number of police brutality 
incidences as the amount of funding goes up. Furthermore, we can see that
the correlation between these two values is a -0.98 which indicates there 
is a extremely strong linear relationship (in this case it is a negative one).


## Problem 2: Linear regression (30 points)

a. Perform a simple linear regression to answer the question of interest. To do this, name your linear model "reg.output" and write the summary of the regression by using "summary(reg.output)". 

```{r, eval=TRUE}
# Remember to remove eval=FALSE!!
reg.output <- lm(formula = dat$po.brut ~ dat$funds, data = dat)
summary(reg.output)
```

__Your answer here.__

b. Report the estimated coefficient, standard error, and p-value of the slope. Is the relationship between funds and incidents statistically significant? Explain.

The estimated coefficient for the slope is -0.367 while the estimated coefficient
for the intercept is 40.54. The p-value for the slope is less than 2 x 10^-16 
(this is the same of the intercept as well). These slope has a standard error
of 0.0045 which means that regression we have is very close to the actual value.
This means that there exists a linear relationship between the funds and the incidents statically as each of these p-values are less than 0.05. This means that we can 
reject the null hypothesis and assert that there is a linear relationship.

c. Draw a scatterplot of po.brut (y-axis) and funds (x-axis). Right below your plot command, use abline to draw the fitted regression line, like this:
```{r, fig.width=4, fig.height=4, eval=TRUE}
# Remember to remove eval=FALSE!!
plot(dat$funds, dat$po.brut,  main="Relationship between Police Department Funding (in millions) and Police Bruality (per year)", xlab="Amount of funding", ylab="Number of police brutality incidences per year")
abline(reg.output, col = "red", lwd=2)
```
Does the line look like a good fit? Why or why not?

For the middle values, the line follows the data precisely. However if we look 
at the the tail-end of both sides of the data we can see that it isn't as precise
anymore. I believe that this line is an okay fit given it can follow the distribution
on average but when it gets to parts of the data where we do not have as much
information (the tail ends), the predicted value isn't as correct.

d. Are the four assumptions of linear regression satisfied? To answer this, draw the relevant plots. (Write a maximum of one sentence per assumption.) If not, what might you try to do to improve this (if you had more time)?

```{r}
plot(dat$funds, reg.output$residuals, main="Residuals vs. x", xlab="x, Scaled speed", ylab="Residuals")
abline(h = 0, lty="dashed")
plot(reg.output, which=1)
plot(dat$funds, dat$po.brut, main="Relationship between funding and police brutality", xlab = "Amount of funding", ylab = "Number of police brutality incidences per year")
plot(reg.output, which=3)
plot(reg.output, which=5)
plot(reg.output, which=2)
```

1.Linearly Assumption: This assumption doesn't hold because we can clearly see
that the red line for the graph on residuals vs fitted is not flat and 
the residuals vs x has a pattern, therefore this means there is non constant
variance.

2.Independence assumption : This doesn't hold because there is a pattern
in our graph for residuals vs x.

3.Equal variance assumption : This assumption probably doesn't hold because
our scale-location plot isn't straight and there is a pattern with it.

4.Normal Population Assumption : When we look at our normal qq plot, we
see that the there is a left skew (the left and the right ends of the tail
are lighter and smaller than the normal distribution).

The four assumptions were not satisfied, next time I would try and get more data
or I could try and use a different model that would help us see a better relationship,
if there exists one.

e. Answer the question of interest based on your analysis.

Because our assumptions do not hold, we can't use this linear regression
model to determine if more funding will lead to a decrease of police brutality.
This means the results that we obtained to reject the null hypothesis can not
be used and it is inconclusive. The results we obtained are misleading.

## Problem 3: Data ethics (10 points)

Describe the dataset. Considering our lecture on data ethics, what concerns do you have about the dataset? Once you perform your analysis to answer the question of interest using this dataset, what concerns might you have about the results?

This dataset gives us the funding a police department receives and the amount
of police brutality that occurs. From our data ethics lecture, an issue we could 
come across is correlation doesn't imply causation. So even though we have a very
high correlation, this doesn't necessary mean that the two variables are related.
People could potentially look at this correlation and see the regression without realizing 
that our assumption didn't hold.

This matters because people could use this data or data like this to push a false
narrative that could damage people. This is an important example to see that 
we have to check everything because if we forget and show some false information 
it could cause people to believe ideas that are not true.

# Assignment 3

Load the data.
```{r}
library(readr)
library(knitr)
dat.crime <- read_delim("crime_simple.txt", delim = "\t")
```

This is a dataset from a textbook by Brian S. Everitt about crime in the US in 1960. The data originate from the Uniform Crime Report of the FBI and other government sources. The data for 47 states of the USA are given. 

Here is the codebook:

R: Crime rate: # of offenses reported to police per million population

Age: The number of males of age 14-24 per 1000 population

S: Indicator variable for Southern states (0 = No, 1 = Yes)

Ed: Mean of years of schooling x 10 for persons of age 25 or older

Ex0: 1960 per capita expenditure on police by state and local government

Ex1: 1959 per capita expenditure on police by state and local government

LF: Labor force participation rate per 1000 civilian urban males age 14-24

M: The number of males per 1000 females

N: State population size in hundred thousands

NW: The number of non-whites per 1000 population

U1: Unemployment rate of urban males per 1000 of age 14-24

U2: Unemployment rate of urban males per 1000 of age 35-39

W: Median value of transferable goods and assets or family income in tens of $

X: The number of families per 1000 earning below 1/2 the median income


We are interested in checking whether the reported crime rate (# of offenses reported to police per million population) and the average education (mean number of years of schooling for persons of age 25 or older) are related. 


1. How many observations are there in the dataset? To what does each observation correspond?

```{r}
names(dat.crime)
dim(dat.crime)
```



Excluding the first row (because it tells us the names of the columns), there are 
46 observations. Each column corresponds to "R"  "Age" "S"   "Ed"  "Ex0" "Ex1" "LF"  "M"   "N"   "NW"  "U1"  "U2"  "W"   "X"  respectively. In our codebook we see that
each of the observations is a different data that was recorded.

2. Draw a scatterplot of the two variables. Calculate the correlation between the two variables. Can you come up with an explanation for this relationship?

R ED

```{r, fig.width=6, fig.height=4}
plot(dat.crime$Ed, dat.crime$R, main="Relationship between reported crime rate and mean years of schooling", xlab = "Mean of years Schooling x10", ylab = "Reported Crime Rate")
cor(dat.crime$Ed, dat.crime$R)
```

The correlation between these two variables is 0.3328349. I don't think
this correlation is high enough for us to draw a particular conclusion yet. 

3. Regress reported crime rate (y) on average education (x) and call this linear model `crime.lm` and write the summary of the regression by using this code, which makes it look a little nicer `{r, eval=FALSE} kable(summary(crime.lm)$coef, digits = 2)`.

```{r, eval=TRUE} 
# Remember to remove eval=FALSE above!

crime.lm <- lm(formula = dat.crime$R ~ dat.crime$Ed, data = dat.crime)
kable(summary(crime.lm)$coef, digits = 2)
```
This regression has a slope of 1.12 which means for every unit of education
that increases the reported crime increases by 1.12. Additionally, we see
that the error for this is 0.49 (which is how much is varys from the average).
Additionally we have a p value of about 2% (which means that this is significant)
and our regression is 2.29 standard deviations away from the 0. Another thing
to take note of is the fact that our intercept estimate is quite different in
comparison to our slope. The estimate is larger as well as the error. Because 
of this our P value is not a significant one for the intercept.


4. Are the four assumptions of linear regression satisfied?To answer this, draw the relevant plots. (Write a maximum of one sentence per assumption.)

```{r} 
plot(dat.crime$Ed, crime.lm$residuals, ylim=c(-15,15), main="Residuals vs. x", xlab="x, Scaled speed", ylab="Residuals")
abline(h = 0, lty="dashed")
plot(crime.lm, which=1)
plot(dat.crime$Ed, dat.crime$R, main="Relationship between reported crime rate and mean years of schooling", xlab = "Mean of years Schooling x10", ylab = "Reported crime rate")
plot(crime.lm, which=5)
plot(crime.lm, which=2)
```

1.Linearly Assumption: Looking at the first two plots for Residuals vs X and Residuals vs Fitted we see that these have no real pattern for the nodes as the red line is almost 
completely flat.

2. Independence assumption : Looking at Residuals vs X, there seems to be no pattern and 
the nodes seem random.

3.Equal variance assumption : There is a slight unequal amount of variable between
points of x = 92 to x = 100, therefore this assumption doesn't stand.

4. Normal Population Assumption
Our qqplot shows us that our residuals tend to be larger in magnitude and overestimate
(on the right side), and our right tail is lighter than the rest for a normal distribution.



5. Is the relationship between reported crime and average education statistically significant? Report the estimated coefficient of the slope, the standard error, and the p-value. What does it mean for the relationship to be statistically significant?
```{r} 
summary(crime.lm)
```

Our estimated coefficient of slope is 1.1161. Our standard residual error was
37.01 on 45 degrees of freedom. We have a high p value for our intercept and a 
p value lower for our slope. For the the relationship to be statistically
significant means there exists a relationship between reported crime and average
education. 

6. How are reported crime and average education related? In other words, for every unit increase in average education, how does reported crime rate change (per million) per state?

Because we have a significant slope value, we can say that we reject the null.
For every unit increase of average education, the reported crime rate changes
by 1.1161.

7. Can you conclude that if individuals were to receive more education, then crime will be reported more often? Why or why not?

Based of our linear regression, we can conclude that individuals who receive more
education will report crime more often since we obtained a pvalue that was
statistically significant. This tells us that there exists some
relationship (from the slope), which we can see is increasing as a unit of
education increases. However, we need to keep in mind that this test doesn't
have enough data points because it is hard to tell if the equal variance and 
the normal population assumption pass. 


# Assignment 4
1. Installed tidyverse and imported it.
```{r, eval=TRUE}
library(tidyverse)
library(ggplot2)
```

2. run mpg dataframe
```{r}
mpg
```
displ, a car’s engine size, in litres.

hwy, a car’s fuel efficiency on the highway, in miles per gallon (mpg). A car with a low fuel efficiency consumes more fuel than a car with a high fuel efficiency when they travel the same distanc

3. Creating a ggplot
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))
```

The plot shows a negative relationship between engine size (displ) and fuel efficiency (hwy). In other words, cars with big engines use more fuel. Does this confirm or refute your hypothesis about fuel efficiency and engine size?

With ggplot2, you begin a plot with the function ggplot(). ggplot() creates a coordinate system that you can add layers to. The first argument of ggplot() is the dataset to use in the graph. So ggplot(data = mpg) creates an empty graph, but it’s not very interesting so I’m not going to show it here.

You complete your graph by adding one or more layers to ggplot(). The function geom_point() adds a layer of points to your plot, which creates a scatterplot. ggplot2 comes with many geom functions that each add a different type of layer to a plot. You’ll learn a whole bunch of them throughout this chapter.

Each geom function in ggplot2 takes a mapping argument. This defines how variables in your dataset are mapped to visual properties. The mapping argument is always paired with aes(), and the x and y arguments of aes() specify which variables to map to the x and y axes. ggplot2 looks for the mapped variables in the data argument, in this case, mpg.

This can be represented in the following template.

ggplot(data = <DATA>) + 
  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))
  
```{r}

dim(mpg)

?mpg

plot(mpg$hwy, mpg$cyl)
```
1. I see nothing
2. There are 234 rows and 11 columns
3. drv is the type of drive train where f = front-wheel drive, r = rear wheel drive, 4 = 4wd
4. Its not useful because it is numerical

4. 
```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, color = class))

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, size = class))
#> Warning: Using size for a discrete variable is not advised.

# Left
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, alpha = class))

# Right
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy, shape = class))

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy), color = "blue")
```
We can use aes to change how our plots look. This refers to aesthetics and 
ggplot will change it for us.

shape changes shape (only does 6 groups). alpha changes transparency. 
color changes color

You can also change the color manually.

Common problems are that people will put the + in the wrong place like 
here: ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy))

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_wrap(~ class, nrow = 2)

ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) + 
  facet_grid(drv ~ cyl)
```

One way to add additional variables is with aesthetics. Another way, particularly useful for categorical variables, is to split your plot into facets, subplots that each display one subset of the data.

To facet your plot by a single variable, use facet_wrap(). The first argument of facet_wrap() should be a formula, which you create with ~ followed by a variable name (here “formula” is the name of a data structure in R, not a synonym for “equation”). The variable that you pass to facet_wrap() should be discrete.

We can add a combination of two variable by using facet_grid.

```{r}
# left
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy))

# right
ggplot(data = mpg) + 
  geom_smooth(mapping = aes(x = displ, y = hwy))

ggplot(data = mpg) + 
  geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv))
```
We can also work with geometric objects in our plot to help with visualization.
Additionally we can also change the linetype so it is different for each 
unique value.

ggplot provides over 40 geom plots that we can use to help with data visualization

```{r}
ggplot(data = mpg) +
  geom_smooth(mapping = aes(x = displ, y = hwy))
              
ggplot(data = mpg) +
  geom_smooth(mapping = aes(x = displ, y = hwy, group = drv))
    
ggplot(data = mpg) +
  geom_smooth(
    mapping = aes(x = displ, y = hwy, color = drv),
    show.legend = FALSE
  )
```
We can also display multiple geom on a single plot.

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy)) +
  geom_smooth(mapping = aes(x = displ, y = hwy))

ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point() + 
  geom_smooth()

ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point(mapping = aes(color = class)) + 
  geom_smooth()

ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + 
  geom_point(mapping = aes(color = class)) + 
  geom_smooth(data = filter(mpg, class == "subcompact"), se = FALSE)
```
So we can change mapping such that there are layer of aesthetics (helps us
specify data in labels). We can also modify the plot such that our
object will only mend to some of the data.

```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut))

ggplot(data = diamonds) + 
  stat_count(mapping = aes(x = cut))
```


A simple barplot where the data is grouped by cut. You can use stat_count
and geom_bar interchangably since geom_bar has a default stat_count.

```{r}
demo <- tribble(
  ~cut,         ~freq,
  "Fair",       1610,
  "Good",       4906,
  "Very Good",  12082,
  "Premium",    13791,
  "Ideal",      21551
)

ggplot(data = demo) +
  geom_bar(mapping = aes(x = cut, y = freq), stat = "identity")
```
You can also change the stat count.


```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, y = stat(prop), group = 1))
```

You might want to override the default mapping from transformed variables to aesthetics. For example, you might want to display a bar chart of proportion, rather than count:

```{r}
ggplot(data = diamonds) + 
  stat_summary(
    mapping = aes(x = cut, y = depth),
    fun.min = min,
    fun.max = max,
    fun = median
  )
```

we can do it for a transformation on statisitcs aswell where we take a stat
summary and take the specific val of each category.

```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, colour = cut))
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = cut))
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity))

ggplot(data = diamonds, mapping = aes(x = cut, fill = clarity)) + 
  geom_bar(alpha = 1/5, position = "identity")
ggplot(data = diamonds, mapping = aes(x = cut, colour = clarity)) + 
  geom_bar(fill = NA, position = "identity")
```
Now, we can change the position such that they can either overlap on each 
other and see something special about the data.

```{r}
ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "fill")

ggplot(data = diamonds) + 
  geom_bar(mapping = aes(x = cut, fill = clarity), position = "dodge")
```
We can use things like position to see proportion to data or dodge to overlap 
them. 

```{r}
ggplot(data = mpg) + 
  geom_point(mapping = aes(x = displ, y = hwy), position = "jitter")
```
We can change our data such that their isn't overplotting such that the arrangement
is now easier to see.


The template for all this is:

ggplot(data = <DATA>) + 
  <GEOM_FUNCTION>(
     mapping = aes(<MAPPINGS>),
     stat = <STAT>, 
     position = <POSITION>
  ) +
  <COORDINATE_FUNCTION> +
  <FACET_FUNCTION>
  
"To see this, let’s add position adjustments, stats, coordinate systems, and faceting to our code template:"

## Graphics for communication

```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth(se = FALSE) +
  labs(title = "Fuel efficiency generally decreases with engine size")

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth(se = FALSE) +
  labs(
    title = "Fuel efficiency generally decreases with engine size",
    subtitle = "Two seaters (sports cars) are an exception because of their light weight",
    caption = "Data from fueleconomy.gov"
  )

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_smooth(se = FALSE) +
  labs(
    x = "Engine displacement (L)",
    y = "Highway fuel economy (mpg)",
    colour = "Car type"
  )

df <- tibble(
  x = runif(10),
  y = runif(10)
)
ggplot(df, aes(x, y)) +
  geom_point() +
  labs(
    x = quote(sum(x[i] ^ 2, i == 1, n)),
    y = quote(alpha + beta + frac(delta, theta))
  )
```
We can put labels in places like the title. You can also add them 
as subtitles, captions, x, and y. Labels can even be written as mathematical
equations.

You can take it a step further and make annotations for specific points
```{r}
best_in_class <- mpg %>%
  group_by(class) %>%
  filter(row_number(desc(hwy)) == 1)

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_text(aes(label = model), data = best_in_class)

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_label(aes(label = model), data = best_in_class, nudge_y = 2, alpha = 0.5)

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_point(size = 3, shape = 1, data = best_in_class) +
  ggrepel::geom_label_repel(aes(label = model), data = best_in_class)
```
In this graph, we label the cars that are the best in their class in terms of 
mpg.

The label is hard to read from one another, so we can modify it such that
it is easier to see. Additionally, if you use ggrepel then you can
make it so the labels are not right on top of each other.

```{r}
class_avg <- mpg %>%
  group_by(class) %>%
  summarise(
    displ = median(displ),
    hwy = median(hwy)
  )
#> `summarise()` ungrouping output (override with `.groups` argument)

ggplot(mpg, aes(displ, hwy, colour = class)) +
  ggrepel::geom_label_repel(aes(label = class),
    data = class_avg,
    size = 6,
    label.size = 0,
    segment.color = NA
  ) +
  geom_point() +
  theme(legend.position = "none")

label <- mpg %>%
  summarise(
    displ = max(displ),
    hwy = max(hwy),
    label = "Increasing engine size is \nrelated to decreasing fuel economy."
  )

ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  geom_text(aes(label = label), data = label, vjust = "top", hjust = "right")

label <- tibble(
  displ = Inf,
  hwy = Inf,
  label = "Increasing engine size is \nrelated to decreasing fuel economy."
)

ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  geom_text(aes(label = label), data = label, vjust = "top", hjust = "right")

```
We can turn the legend off and then put annotations near the colors 
such that they represent what each car type is. Additionally, you 
can just add a single label to the plot and then write what you want.

If you want to place text exactly where the borders are then you can use 
INF(which can be positive or negative).

There are nine different possibilities for locations that labels can go in (NESW).

Use geom_hline() and geom_vline() to add reference lines. I often make them thick (size = 2) and white (colour = white), and draw them underneath the primary data layer. That makes them easy to see, without drawing attention away from the data.

Use geom_rect() to draw a rectangle around points of interest. The boundaries of the rectangle are defined by aesthetics xmin, xmax, ymin, ymax.

Use geom_segment() with the arrow argument to draw attention to a point with an arrow. Use aesthetics x and y to define the starting location, and xend and yend to define the end location.

```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class))

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  scale_x_continuous() +
  scale_y_continuous() +
  scale_colour_discrete()

ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  scale_y_continuous(breaks = seq(15, 40, by = 5))

ggplot(mpg, aes(displ, hwy)) +
  geom_point() +
  scale_x_continuous(labels = NULL) +
  scale_y_continuous(labels = NULL)
```

Another way we can change how the plot looks for communication is by adjusting
the scales of the plot. ggplot will automatically add the scales behind the
scenes but you can manually change these how you would like.

There are two primary arguments that affect the appearance of the ticks on the axes and the keys on the legend: breaks and labels. Breaks controls the position of the ticks, or the values associated with the keys. Labels controls the text label associated with each tick/key. The most common use of breaks is to override the default choice:

You can also set the values within these scales (labels) to null
so it doesn't share the absolute value of the data but you can still
see some sort of trend that is associated with it.

```{r}
presidential %>%
  mutate(id = 33 + row_number()) %>%
  ggplot(aes(start, id)) +
    geom_point() +
    geom_segment(aes(xend = end, yend = id)) +
    scale_x_date(NULL, breaks = presidential$start, date_labels = "'%y")

```
Collectively axes and legends are called guides. Axes are used for x and y aesthetics; legends are used for everything else.

You can scale the dates such that in this plot it shows us when the president start and ended
their term.

Note that the specification of breaks and labels for date and datetime scales is a little different:

date_labels takes a format specification, in the same form as parse_datetime().

date_breaks (not shown here), takes a string like “2 days” or “1 month”.

```{r}
base <- ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class))

base + theme(legend.position = "left")
base + theme(legend.position = "top")
base + theme(legend.position = "bottom")
base + theme(legend.position = "right") # the default
```

You can edit the legend by changing its position on the graph, or you
can set the legend position to none such that it won't be anywhere.

```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(colour = class)) +
  geom_smooth(se = FALSE) +
  theme(legend.position = "bottom") +
  guides(colour = guide_legend(nrow = 1, override.aes = list(size = 4)))
#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'
```
To control the display of individual legends, use guides() along with guide_legend() or guide_colourbar(). The following example shows two important settings: controlling the number of rows the legend uses with nrow, and overriding one of the aesthetics to make the points bigger. This is particularly useful if you have used a low alpha to display many points on a plot.

```{r}
ggplot(diamonds, aes(carat, price)) +
  geom_bin2d()

ggplot(diamonds, aes(log10(carat), log10(price))) +
  geom_bin2d()

ggplot(diamonds, aes(carat, price)) +
  geom_bin2d() + 
  scale_x_log10() + 
  scale_y_log10()
```
We can also change the scale of our plot. However it could be disadvantageous
since the labels are now the transformed version of the data. This last
example is the same but the axes are labeled on the original data scale.

```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = drv))

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = drv)) +
  scale_colour_brewer(palette = "Set1")

ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = drv, shape = drv)) +
  scale_colour_brewer(palette = "Set1")
```
You can also change the color or these points to help people who may have
colorblindness. We can also edit the shape of these points in the graph.

The ColorBrewer scales are documented online at http://colorbrewer2.org/ and 
made available in R via the RColorBrewer package, by Erich Neuwirth.

```{r}
presidential %>%
  mutate(id = 33 + row_number()) %>%
  ggplot(aes(start, id, colour = party)) +
    geom_point() +
    geom_segment(aes(xend = end, yend = id)) +
    scale_colour_manual(values = c(Republican = "red", Democratic = "blue"))

df <- tibble(
  x = rnorm(10000),
  y = rnorm(10000)
)
ggplot(df, aes(x, y)) +
  geom_hex() +
  coord_fixed()

ggplot(df, aes(x, y)) +
  geom_hex() +
  viridis::scale_fill_viridis() +
  coord_fixed()
```

You can also set the color manually.

For continuous colour, you can use the built-in scale_colour_gradient() or scale_fill_gradient(). If you have a diverging scale, you can use scale_colour_gradient2(). That allows you to give, for example, positive and negative values different colours. That’s sometimes also useful if you want to distinguish points above or below the mean.

Another option is scale_colour_viridis() provided by the viridis package. It’s a continuous analog of the categorical ColorBrewer scales. The designers, Nathaniel Smith and Stéfan van der Walt, carefully tailored a continuous colour scheme that has good perceptual properties. Here’s an example from the viridis vignette.

```{r}
ggplot(mpg, mapping = aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth() +
  coord_cartesian(xlim = c(5, 7), ylim = c(10, 30))

mpg %>%
  filter(displ >= 5, displ <= 7, hwy >= 10, hwy <= 30) %>%
  ggplot(aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth()

suv <- mpg %>% filter(class == "suv")
compact <- mpg %>% filter(class == "compact")

ggplot(suv, aes(displ, hwy, colour = drv)) +
  geom_point()

ggplot(compact, aes(displ, hwy, colour = drv)) +
  geom_point()

x_scale <- scale_x_continuous(limits = range(mpg$displ))
y_scale <- scale_y_continuous(limits = range(mpg$hwy))
col_scale <- scale_colour_discrete(limits = unique(mpg$drv))

ggplot(suv, aes(displ, hwy, colour = drv)) +
  geom_point() +
  x_scale +
  y_scale +
  col_scale

ggplot(compact, aes(displ, hwy, colour = drv)) +
  geom_point() +
  x_scale +
  y_scale +
  col_scale
```

You can also set the limits on individual scales. Reducing the limits is basically equivalent to subsetting the data. It is generally more useful if you want expand the limits, for example, to match scales across different plots. For example, if we extract two classes of cars and plot them separately, it’s difficult to compare the plots because all three scales (the x-axis, the y-axis, and the colour aesthetic) have different ranges.

One way to overcome this problem is to share scales across multiple plots, training the scales with the limits of the full data. In this particular case, you could have simply used faceting, but this technique is useful more generally, if for instance, you want spread plots over multiple pages of a report.

```{r}
ggplot(mpg, aes(displ, hwy)) +
  geom_point(aes(color = class)) +
  geom_smooth(se = FALSE) +
  theme_bw()
```

We can set themes for our plot as well and there are 8 built in ones for gg.plot.

```{r}
ggplot(mpg, aes(displ, hwy)) + geom_point()
ggsave("my-plot.pdf")
#> Saving 7 x 4.33 in image
```

There are two main ways to get your plots out of R and into your final write-up: ggsave() and knitr. ggsave() will save the most recent plot to disk.

But if you don't specify the width and the height it will just take from what
the current plotting has. So for reproducability it is important to do these.


There are five main options that control figure sizing: fig.width, fig.height, fig.asp, out.width and out.height

They said they only use three out of the five : 

"I find it most aesthetically pleasing for plots to have a consistent width. To enforce this, I set fig.width = 6 (6") and fig.asp = 0.618 (the golden ratio) in the defaults. Then in individual chunks, I only adjust fig.asp.

I control the output size with out.width and set it to a percentage of the line width. I default to out.width = "70%" and fig.align = "center". That give plots room to breathe, without taking up too much space.

To put multiple plots in a single row I set the out.width to 50% for two plots, 33% for 3 plots, or 25% to 4 plots, and set fig.align = "default". Depending on what I’m trying to illustrate (e.g. show data or show plot variations), I’ll also tweak fig.width, as discussed below."

If you find that you’re having to squint to read the text in your plot, you need to tweak fig.width. If fig.width is larger than the size the figure is rendered in the final doc, the text will be too small; if fig.width is smaller, the text will be too big. You’ll often need to do a little experimentation to figure out the right ratio between the fig.width and the eventual width in your document. 

If you want to make sure the font size is consistent across all your figures, whenever you set out.width, you’ll also need to adjust fig.width to maintain the same ratio with your default out.width. 

He said the following: 
"I recommend setting fig.show = "hold" so that plots are shown after the code. This has the pleasant side effect of forcing you to break up large blocks of code with their explanations.

To add a caption to the plot, use fig.cap. In R Markdown this will change the figure from inline to “floating”."

# Final 

```{r}
library(usmap)
library(ggplot2)

table12 <- read.csv(file="table12.csv")
table11 <- read.csv(file="table11.csv")
```

## R Markdown


```{r}
summary(table12)

table12$Number.of.participating.agencies <- 
  as.integer(gsub(",", "", table12$Number.of.participating.agencies))
table12$Population.covered <- 
  as.integer(gsub(",", "", table12$Population.covered))
table12$Total.number.of.incidents.reported <-
  as.integer(gsub(",","",table12$Total.number.of.incidents.reported))

summary(table12)
```

<First check if there is a correlation with the number of agencies participating
and the population>
```{r pressure, echo=FALSE}
options(scipen=999)
plot(table12$Population.covered, table12$Number.of.participating.agencies, 
     xlab="Population", ylab="Number of Agencies")
plot(table12$Population.covered, table12$Number.of.participating.agencies, 
     xlab="Population", ylab="Number of Agencies",
     xlim = range(0, 12000000)) 

cor(table12$Population.covered, table12$Number.of.participating.agencies)
```

<See how many of agencies are submitting reports out of number submitting>
<data shows that a very low number agencies are actually submitting reports>
```{r}

percent_participate_table <- data.frame(table12$Participating.state.Federal,
                                  table12$Agencies.submitting.incident.reports,
                                  table12$Number.of.participating.agencies)

colnames(percent_participate_table) <- c('States', 'Agencies Submitting Reports'
                                         , "Agencies Participating")

percent_participate_table = transform(percent_participate_table, 
                            freq = (percent_participate_table$`Agencies Submitting Reports`/percent_participate_table$`Agencies Participating`)*100)

```



<there is a correlation -> Lets see if we can predict the n>
<first lets see if the number of ag>
```{r}
plot(table12$Number.of.participating.agencies, table12$Total.number.of.incidents.reported, 
                                  xlab="Population", ylab="Number of Agencies")
plot(table12$Number.of.participating.agencies, table12$Total.number.of.incidents.reported, 
                                  xlab="Number of agencies", 
                                  ylab="Number of incidents", 
                                  xlim = range(0, 800))

cor(table12$Number.of.participating.agencies, table12$Total.number.of.incidents.reported)
```
<lets see if there is an increase of agencies means an increase of reports>
```{r}
partagent <-table12$Number.of.participating.agencies
reported <-table12$Total.number.of.incidents.reported
lm_agencies_report <- lm(formula = partagent ~ reported,
                         data = table12)
summary(lm_agencies_report)
```

<linear regression assumptions test>
```{r}

plot(partagent, lm_agencies_report$residuals, main="Residuals vs. x", xlab="x, Scaled speed", ylab="Residuals")
abline(h = 0, lty="dashed")
```

```{r}
plot(lm_agencies_report, which=3)
```

```{r}
plot(lm_agencies_report, which=5)
plot(lm_agencies_report, which=2)
```
<a regression isn't valid so we can't say that for every predicted agencies that we get more reports>

<now look at the number of offenses we are seeing across state lines>
```{r}
summary(table11)
```
<total offences in general looking at freq from table 12 to table 11>
```{r}
table11_total = data.frame(table11$Participating.state.Federal, table11$Total.offenses, percent_participate_table$freq)
```

<most common crimes per state>
```{r}
table11_maxperstate <- data.frame(table11$Participating.state.Federal)
table11_maxperstate['max'] <- apply(table11[3:15], 1, max)
table11_maxperstate['highest occuring crimes'] <- colnames(table11[3:15])[max.col(table11[3:15], ties.method = "first")]

table11_total
table11_maxperstate
```

```{r}
incidents <- data.frame(table12$Participating.state.Federal, as.numeric(gsub(",","",table12$Total.number.of.incidents.reported)))
colnames(incidents) <- c('state', 'Total number of incidents')
plot_usmap(data = incidents, values = "Total number of incidents")+scale_fill_continuous(name = "Number of Incidents",low = "white", high ="darkblue", label = scales::comma) + theme(legend.position = "right")
```
```{r}
percent_participate_table <- data.frame(table12$Participating.state.Federal,
                                  table12$Agencies.submitting.incident.reports,
                                  table12$Number.of.participating.agencies)

colnames(percent_participate_table) <- c('state', 'Agencies Submitting Reports'
                                         , "Agencies Participating")

percent_participate_table = transform(percent_participate_table, freq = (percent_participate_table$`Agencies Submitting Reports`/percent_participate_table$`Agencies Participating`)*100)


plot_usmap(data = percent_participate_table, values = "freq")+scale_fill_continuous(name = "Percent of Participating Agencies",low = "white", high ="darkblue", label = scales::comma) + theme(legend.position = "right")
```

```{r}

colnames(table11_maxperstate) <- c('state', "max", "incidents")
plot_usmap(data = table11_maxperstate, values = "incidents") + theme(legend.position = "right") + scale_fill_brewer(name = "Incidents with the highest occurrence",type = 'qual', palette = 1) 
```



